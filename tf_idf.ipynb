{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "d1 = 're have abcd or ie usa and and uk and other'\n",
    "d2 = 'buy apple good apple from ms abc at dollar pound from monday-friday how s that'\n",
    "d3 = 'win t eat of the top cake get dollar or usd and a long-term short-term goal'\n",
    "d4 = 'lose pound at or pm or pm or pm or pm on  or '\n",
    "d5 = 'hes a dedicated person dedicate his time to work are sure'\n",
    "d6 = 'be interested in these interesting thing that interest'\n",
    "docs = [d1, d2, d3, d4, d5, d6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(\n",
    "    input='content',                     # default；{‘filename’，‘file’，‘content’}；input must be a list\n",
    "    encoding='utf-8',                    # default; same options as in str.decode(encoding='utf-8')\n",
    "    decode_error='strict',               # default; same options as in str.decode(errors='strict')\n",
    "    # preprocessing arguments\n",
    "    lowercase=True,                      # default; convert strings to lowercase\n",
    "    strip_accents='unicode',             # default=None; remove accents; 'unicode' is slower but universal\n",
    "    preprocessor=None,                   # default\n",
    "    # tokenization arguments\n",
    "    analyzer='word',                     # default;{'word','char','char_wb'}\n",
    "    token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',  # default; equivalent to using a nltk.RegexpTokenizer()\n",
    "    tokenizer=None,                      # default; can use a nltk tokenizer\n",
    "    # vocabulary arguments\n",
    "    stop_words=None,                     # default=None; try stop_words='english' for example\n",
    "    ngram_range=(1,2),                   # default=(1,1)\n",
    "    min_df=1,                            # default=1  ; int or [0.0, 1.0]; ignore terms with a doc-freq < cutoff\n",
    "    max_df=0.8,                          # default=1.0; [0.0, 1.0] or int; ignore terms with a doc-freq > cutoff; 整数是多少个，小数是百分比\n",
    "    max_features=None,                   # default; keep only the top max_features ordered by term-freq; 保持满足上述条件的词汇量大小\n",
    "    vocabulary=None,                     # default; if provided, max_df, min_df ,max_features are ignored\n",
    "    # TF - IDF adjustment arguments\n",
    "    binary=False,                        # default; if True, all non-zero term counts are set to 1\n",
    "    sublinear_tf=True,                   # default; if True, use 1 + log(tf) for non-zeros; else use tf\n",
    "    use_idf=True,                        # default; if True, enable IDF re-weighting\n",
    "    smooth_idf=True,                     # default; if True, use 1 + log((N_docs+1)/(df+1)), else use 1 + log(N_docs/df)\n",
    "    norm='l2'                            # default; if True, preform post TF-IDF normalization such that output row has unit norm\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let's try this out: aigue beonte\n",
      "['let', 'try', 'this', 'out', 'aigue', 'beonte']\n",
      "['let', 'try', 'this', 'out', 'aigue', 'beonte', 'let try', 'try this', 'this out', 'out aigue', 'aigue beonte']\n"
     ]
    }
   ],
   "source": [
    "# understand TfidfVectorizer - before performing vec.fit_transform()\n",
    "preprocessor = vec.build_preprocessor()       #lower strings & remove accents\n",
    "print(preprocessor(\"Let's TRY this OUT: aigué béonté\"))\n",
    "\n",
    "tokenizer = vec.build_tokenizer()        #tokenize documents\n",
    "print(tokenizer(\"let's try this out: aigue beonte\"))\n",
    "\n",
    "analyzer = vec.build_analyzer()      # preprocess > tokenize > remove > stopwords > get n-grams > prune vocabulary\n",
    "print(analyzer(\"let's try this out: aigue beonte\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin\\desktop\\textclassification\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "set()"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understand TfidfVectorizer - after performing vec.fit_transform()\n",
    "document_term_matrix = vec.fit_transform(docs)  # scipy sparse csr matrix\n",
    "idx2term = vec.get_feature_names()\n",
    "term2idx = vec.vocabulary_\n",
    "# print(idx2term)\n",
    "# print(term2idx)\n",
    "vec.get_stop_words()\n",
    "vec.stop_words_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        abc    abc at      abcd   abcd or       and   and and  and long  \\\n",
      "0  0.000000  0.000000  0.221183  0.221183  0.380632  0.221183  0.000000   \n",
      "1  0.184355  0.184355  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.148019  0.000000  0.180508   \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "5  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "   and other    and uk    apple  ...        uk    uk and       usa   usa and  \\\n",
      "0   0.221183  0.221183  0.00000  ...  0.221183  0.221183  0.221183  0.221183   \n",
      "1   0.000000  0.000000  0.31214  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "2   0.000000  0.000000  0.00000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.000000  0.000000  0.00000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "4   0.000000  0.000000  0.00000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "5   0.000000  0.000000  0.00000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "        usd   usd and       win   win eat      work  work are  \n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "2  0.180508  0.180508  0.180508  0.180508  0.000000  0.000000  \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4  0.000000  0.000000  0.000000  0.000000  0.229416  0.229416  \n",
      "5  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[6 rows x 116 columns]\n"
     ]
    }
   ],
   "source": [
    "document_term_matrix = pd.DataFrame(document_term_matrix.todense())  # convert DTMatrix to DataFrame\n",
    "document_term_matrix.columns = vec.get_feature_names()\n",
    "print(document_term_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}