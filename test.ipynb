{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                      Phrase\nSentiment                                                   \n1          A series of escapades demonstrating the adage ...\n2                                                   A series\n2                                                   A series\n2                                                   A series\n2                                                   A series\n...                                                      ...\n3                                                          A\n3                                                          A\n4                                                     series\n4                                                     series\n3                                                          A\n\n[124848 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Phrase</th>\n    </tr>\n    <tr>\n      <th>Sentiment</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>A series of escapades demonstrating the adage ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A series</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A series</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A series</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A series</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>series</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>series</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A</td>\n    </tr>\n  </tbody>\n</table>\n<p>124848 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score,accuracy_score,recall_score,precision_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('data/train.tsv',sep='\\t')\n",
    "x_train,x_test,y_train,y_test = train_test_split(data['Phrase'],data['Sentiment'],test_size=0.2,random_state=42,shuffle=False)\n",
    "df = pd.DataFrame(x_train,y_train)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 文本清洗\n",
    "class TextCleaner(object):\n",
    "    d1 = \"We're having a.b.c.d or i.e. u.s.a and 16.2 and U.K. and others.\"\n",
    "    d2 = \"We buy 12 apples, good apples, from Ms. ABC at 16.2 dollars/pound 24/7 from Monday-Friday. How's that?\"\n",
    "    d3 = \"1. I won't eat 1/12 of the #1 top cakes. I got 1.2 dollars or 1.2usd and a long-term/short-term goal\"\n",
    "    d4 = \"I lost 22 pounds at 9:30, 11:05 or 1:30pm or 2pm or 3pm or 1-2pm on 2016-01-02 or 01-02-2016.\"\n",
    "    d5 = \"  --He's a dedicated person. \\t He dedicated his time to work! \\n --are you sure?\"\n",
    "    d6 = \"I am interested in these interesting things that interested me.\"\n",
    "    sample_docs = [d1, d2, d3, d4, d5, d6]\n",
    "\n",
    "    def __init__(self, option='none'):\n",
    "        self.option = option\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def lemmatize(self, text):\n",
    "        tuples = [(str(token), token.lemma_) for token in self.nlp(text)]\n",
    "        text = ' '.join([lemma if lemma != '-PRON-' else token for token, lemma in tuples])\n",
    "        text = re.sub(' ([_-]) ', '\\\\1', text)  # 'short - term' -> 'short-term'\n",
    "        return text\n",
    "\n",
    "    def text_cleanner(self, text):\n",
    "        stopwords = set(['i', 'I', 'you', 'he', 'she', 'it', 'we', 'they', 'am', 'is', 'are'])\n",
    "        text = text.lower()  # 1.lower strings\n",
    "        text = text.replace(\"'s\", 's')  # 2.handle contractions\n",
    "        text = re.sub('([a-zA-Z])\\\\.([a-zA-Z])\\\\.*', '\\\\1\\\\2', text)  # 3. handle abbreviations\n",
    "        text = re.sub('\\\\d+', '', text)  # 4. handle numbers\n",
    "        text = re.sub('[^a-zA-Z0-9\\\\s_-]', ' ', text)  # 5. remove punctuations\n",
    "        if 'lemma' in self.option:\n",
    "            text = self.lemmatize(text)  # 6. perform lemmatization\n",
    "        elif 'stem' in self.option:\n",
    "            text = ' '.join(self.stemmer.stem(x) for x in text.split(' '))  # 6. perform stemming\n",
    "        text = ' '.join(x for x in text.split(' ') if\n",
    "                        x not in stopwords)  # 7. remove stopwords(stopwords类型是set而不是list，因为set的查找速度更快)\n",
    "        text = ' '.join(x.strip('_-') for x in text.split())  # 8. remove spaces,'_' and '-'\n",
    "        return text\n",
    "\n",
    "    def transform(self, docs):  # transformer必须要有fit()和transform(）方法\n",
    "        clean_docs = []\n",
    "        self.fails = []\n",
    "        for doc in tqdm(docs):\n",
    "            try:\n",
    "                clean_docs.append(self.text_cleanner(doc))\n",
    "            except:\n",
    "                self.fails.append(doc)\n",
    "        if len(self.fails) > 0:\n",
    "            print(\"Some documents failed to be converted. Check self.fails for failed documents\")\n",
    "        return clean_docs\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, docs, y=None):\n",
    "        return self.fit(docs,y).transform(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124848/124848 [06:29<00:00, 320.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: 124848 \t Y_train shape: 124848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cleanner = TextCleaner(option='lemma')\n",
    "x_train = cleanner.transform(x_train)\n",
    "# for raw,clean in zip(x_train, x_train):\n",
    "#     # print('original doc:', raw)\n",
    "#     print(clean)\n",
    "#     print()\n",
    "print('X_train shape:', len(x_train), '\\t', 'Y_train shape:', len(y_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Tf-idf特征提取\n",
    "vec = TfidfVectorizer(\n",
    "    input='content',                     # default；{‘filename’，‘file’，‘content’}；input must be a list\n",
    "    encoding='utf-8',                    # default; same options as in str.decode(encoding='utf-8')\n",
    "    decode_error='strict',               # default; same options as in str.decode(errors='strict')\n",
    "    # preprocessing arguments\n",
    "    lowercase=True,                      # default; convert strings to lowercase\n",
    "    strip_accents='unicode',             # default=None; remove accents; 'unicode' is slower but universal\n",
    "    preprocessor=None,                   # default\n",
    "    # tokenization arguments\n",
    "    analyzer='word',                     # default;{'word','char','char_wb'}\n",
    "    token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',  # default; equivalent to using a nltk.RegexpTokenizer()\n",
    "    tokenizer=None,                      # default; can use a nltk tokenizer\n",
    "    # vocabulary arguments\n",
    "    stop_words=None,                     # default=None; try stop_words='english' for example\n",
    "    ngram_range=(1,2),                   # default=(1,1)\n",
    "    min_df=1,                            # default=1  ; int or [0.0, 1.0]; ignore terms with a doc-freq < cutoff\n",
    "    max_df=0.8,                          # default=1.0; [0.0, 1.0] or int; ignore terms with a doc-freq > cutoff; 整数是多少个，小数是百分比\n",
    "    max_features=None,                   # default; keep only the top max_features ordered by term-freq; 保持满足上述条件的词汇量大小\n",
    "    vocabulary=None,                     # default; if provided, max_df, min_df ,max_features are ignored\n",
    "    # TF - IDF adjustment arguments\n",
    "    binary=False,                        # default; if True, all non-zero term counts are set to 1\n",
    "    sublinear_tf=True,                   # default; if True, use 1 + log(tf) for non-zeros; else use tf\n",
    "    use_idf=True,                        # default; if True, enable IDF re-weighting\n",
    "    smooth_idf=True,                     # default; if True, use 1 + log((N_docs+1)/(df+1)), else use 1 + log(N_docs/df)\n",
    "    norm='l2'                            # default; if True, preform post TF-IDF normalization such that output row has unit norm\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let's try this out: aigue beonte\n",
      "['let', 'try', 'this', 'out', 'aigue', 'beonte']\n",
      "['let', 'try', 'this', 'out', 'aigue', 'beonte', 'let try', 'try this', 'this out', 'out aigue', 'aigue beonte']\n"
     ]
    }
   ],
   "source": [
    "# understand TfidfVectorizer - before performing vec.fit_transform()\n",
    "preprocessor = vec.build_preprocessor()       #lower strings & remove accents\n",
    "print(preprocessor(\"Let's TRY this OUT: aigué béonté\"))\n",
    "\n",
    "tokenizer = vec.build_tokenizer()        #tokenize documents\n",
    "print(tokenizer(\"let's try this out: aigue beonte\"))\n",
    "\n",
    "analyzer = vec.build_analyzer()      # preprocess > tokenize > remove > stopwords > get n-grams > prune vocabulary\n",
    "print(analyzer(\"let's try this out: aigue beonte\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "# understand TfidfVectorizer - after performing vec.fit_transform()\n",
    "document_term_matrix = vec.fit_transform(x_train)  # scipy sparse csr matrix\n",
    "idx2term = vec.get_feature_names_out()\n",
    "term2idx = vec.vocabulary_\n",
    "vec.get_stop_words()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "# term2idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 67.9 GiB for an array with shape (124848, 72958) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m x_train \u001B[38;5;241m=\u001B[39m \u001B[43mvec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m x_test \u001B[38;5;241m=\u001B[39m vec\u001B[38;5;241m.\u001B[39mtransform(x_test)\u001B[38;5;241m.\u001B[39mtoarray()\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx_train shape:\u001B[39m\u001B[38;5;124m'\u001B[39m,x_train\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_train shape:\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mlen\u001B[39m(y_train))\n",
      "File \u001B[1;32mE:\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001B[0m, in \u001B[0;36m_cs_matrix.toarray\u001B[1;34m(self, order, out)\u001B[0m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m order \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1050\u001B[0m     order \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_swap(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcf\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m-> 1051\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_toarray_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (out\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mc_contiguous \u001B[38;5;129;01mor\u001B[39;00m out\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mf_contiguous):\n\u001B[0;32m   1053\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOutput array must be C or F contiguous\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mE:\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:1288\u001B[0m, in \u001B[0;36mspmatrix._process_toarray_args\u001B[1;34m(self, order, out)\u001B[0m\n\u001B[0;32m   1286\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[0;32m   1287\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1288\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 67.9 GiB for an array with shape (124848, 72958) and data type float64"
     ]
    }
   ],
   "source": [
    "x_train = vec.fit_transform(x_train).toarray()\n",
    "x_test = vec.transform(x_test).toarray()\n",
    "print('x_train shape:',x_train.shape, '\\t', 'y_train shape:', len(y_train))\n",
    "print('x_test shape:',x_test.shape, '\\t', 'y_test shape:',len(y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "# 多分类 multi-class classification； 注意和 multi-label 的区别\n",
    "logreg = LogisticRegression(\n",
    "    penalty='l2',                # default; {'l1','l2','elasticnet','none'}\n",
    "    l1_ratio=None,               # range (0,1) used if penalty='elasticnet'\n",
    "    C=1.0,                       # default; inverse of alphs - smaller valves give stronger regularization\n",
    "    multi_class='ovr',   # default='multinomial'; {'auto','ovr','multinomial'} handles multi-class only\n",
    "    solver='liblinear',          # default; {'liblinear','lbfgs','newton-cg','sag','saga'}\n",
    "    max_iter=100,                # default;\n",
    "    tol=1e-4,                    # default; tolerance for stopping criteria\n",
    ")\n",
    "# 'ovr' fits multiple binary classifiers while 'multinomial' fits one multi-class classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5)\n",
      "(1000,)\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2]\n",
      "[[0.01571291 0.1132261  0.73214219 0.09021348 0.04870532]\n",
      " [0.01571291 0.1132261  0.73214219 0.09021348 0.04870532]\n",
      " [0.016658   0.08597204 0.71821246 0.11591299 0.06324451]\n",
      " ...\n",
      " [0.01747589 0.10201017 0.73064078 0.11382994 0.03604323]\n",
      " [0.01747589 0.10201017 0.73064078 0.11382994 0.03604323]\n",
      " [0.01747589 0.10201017 0.73064078 0.11382994 0.03604323]]\n"
     ]
    }
   ],
   "source": [
    "logreg.fit(x_train, y_train[:1000])\n",
    "y_prob = logreg.predict_proba(x_test)  # 先预测概率probability\n",
    "print(y_prob.shape);y_prob   # 每行概率相加【\n",
    "y_pred = logreg.predict(x_test)\n",
    "print(y_pred.shape);y_pred\n",
    "print(y_prob.argmax(1))\n",
    "print(y_prob)\n",
    "# print('accuracy:',accuracy_score(Y_test,y_pred).round(4))\n",
    "# print('precision-macro:',precision_score(Y_test,y_pred,average='macro').round(4))\n",
    "# print('precision-micro:',precision_score(Y_test,y_pred,average='micro').round(4))\n",
    "# print('precision-weighted:',precision_score(Y_test,y_pred,average='weighted').round(4))\n",
    "# print('recall-macro:',recall_score(Y_test,y_pred,average='macro').round(4))\n",
    "# print('recall-micro:',recall_score(Y_test,y_pred,average='micro').round(4))\n",
    "# print('recall-weighted:',recall_score(Y_test,y_pred,average='weighted').round(4))\n",
    "# print('f1-macro:',f1_score(Y_test,y_pred,average='macro').round(4))\n",
    "# print('f1-micro:',f1_score(Y_test,y_pred,average='micro').round(4))\n",
    "# print('f1-weighted:',f1_score(Y_test,y_pred,average='weighted').round(4))\n",
    "# # or\n",
    "# print(classification_report(Y_test,y_pred,digits=4))\n",
    "# pdat = pd.DataFrame(classification_report(Y_test,y_pred,digits=4,output_dict=True)).round(4).T\n",
    "# print(pdat)\n",
    "# print(pdat[:4].mean(0).round(4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({2: 993, 3: 6, 1: 1})"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_prob.argmax(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}