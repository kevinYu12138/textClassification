{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer\n",
    "\n",
    "nlp =spacy.load(\"en_core_web_sm\")  # 加载预训练的模型\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "d1 = \"We're having a.b.c.d or i.e. u.s.a and 16.2 and U.K. and others.\"\n",
    "d2 = \"We buy 12 apples, good apples, from Ms. ABC at 16.2 dollars/pound 24/7 from Monday-Friday. How's that?\"\n",
    "d3 = \"1. I won't eat 1/12 of the #1 top cakes. I got 1.2 dollars or 1.2usd and a long-term/short-term goal\"\n",
    "d4 = \"I lost 22 pounds at 9:30, 11:05 or 1:30pm or 2pm or 3pm or 1-2pm on 2016-01-02 or 01-02-2016.\"\n",
    "d5 = \"  --He's a dedicated person. \\t He dedicated his time to work! \\n --are you sure?\"\n",
    "d6 = \"I am interested in these interesting things that interested me.\"\n",
    "docs = [d1, d2, d3, d4, d5, d6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 定义文本预处理/清洗函数\n",
    "def lemmatize(text):\n",
    "    tuples = [(str(token), token.lemma_ ) for token in nlp(text)]\n",
    "    # print(tuples)\n",
    "    text = ' '.join([lemma if lemma != '-PRON-' else token for token, lemma in tuples])\n",
    "    text = re.sub(' ([_-]) ', '\\\\1', text)  # 'short - term' -> 'short-term'\n",
    "    return text\n",
    "\n",
    "# print(lemmatize(d5))\n",
    "# print(lemmatize(d6))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 去除停用词\n",
    "def text_cleaner(text, option='none'):\n",
    "    stopwords = set(['i', 'I', 'you', 'he', 'she', 'it', 'we', 'they', 'am', 'is', 'are'])\n",
    "    text = text.lower()                                                # 1.lower strings\n",
    "    text = text.replace(\"'s\", 's')                                     # 2.handle contractions\n",
    "    text = re.sub('([a-zA-Z])\\\\.([a-zA-Z])\\\\.*', '\\\\1\\\\2', text)       # 3. handle abbreviations\n",
    "    text = re.sub('\\\\d+', '', text)                                    # 4. handle numbers\n",
    "    text = re.sub('[^a-zA-Z0-9\\\\s_-]', ' ', text)                      # 5. remove punctuations\n",
    "    if 'lemma' in option:\n",
    "        text = lemmatize(text)                                         # 6. perform lemmatization\n",
    "    elif 'stem' in option:\n",
    "        text = ' '.join(stemmer.stem(x) for x in text.split(' '))      # 6. perform stemming\n",
    "    text = ' '.join(x for x in text.split(' ') if x not in stopwords)  # 7. remove stopwords(stopwords类型是set而不是list，因为set的查找速度更快)\n",
    "    text = ' '.join(x.strip('_-') for x in text.split())            # 8. remove spaces,'_' and '-'\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original doc: We're having a.b.c.d or i.e. u.s.a and 16.2 and U.K. and others.\n",
      "cleaned doc: re have abcd or ie usa and and uk and other\n",
      "\n",
      "original doc: We buy 12 apples, good apples, from Ms. ABC at 16.2 dollars/pound 24/7 from Monday-Friday. How's that?\n",
      "cleaned doc: buy apple good apple from ms abc at dollar pound from monday-friday how s that\n",
      "\n",
      "original doc: 1. I won't eat 1/12 of the #1 top cakes. I got 1.2 dollars or 1.2usd and a long-term/short-term goal\n",
      "cleaned doc: win t eat of the top cake get dollar or usd and a long-term short-term goal\n",
      "\n",
      "original doc: I lost 22 pounds at 9:30, 11:05 or 1:30pm or 2pm or 3pm or 1-2pm on 2016-01-02 or 01-02-2016.\n",
      "cleaned doc: lose pound at or pm or pm or pm or pm on  or \n",
      "\n",
      "original doc:   --He's a dedicated person. \t He dedicated his time to work! \n",
      " --are you sure?\n",
      "cleaned doc: hes a dedicated person dedicate his time to work are sure\n",
      "\n",
      "original doc: I am interested in these interesting things that interested me.\n",
      "cleaned doc: be interested in these interesting thing that interest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 文本清洗\n",
    "# option = 'none'\n",
    "option = 'lemmatization'\n",
    "# option = 'stemming'\n",
    "\n",
    "clean_docs = [text_cleaner(doc, option) for doc in docs]\n",
    "for raw, clean in zip(docs, clean_docs):\n",
    "    print('original doc:', raw)\n",
    "    print('cleaned doc:', clean)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 自定义sklearn的transformer class\n",
    "# from sklearn.base import TransformerMixin, BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TextCleaner(object):\n",
    "    d1 = \"We're having a.b.c.d or i.e. u.s.a and 16.2 and U.K. and others.\"\n",
    "    d2 = \"We buy 12 apples, good apples, from Ms. ABC at 16.2 dollars/pound 24/7 from Monday-Friday. How's that?\"\n",
    "    d3 = \"1. I won't eat 1/12 of the #1 top cakes. I got 1.2 dollars or 1.2usd and a long-term/short-term goal\"\n",
    "    d4 = \"I lost 22 pounds at 9:30, 11:05 or 1:30pm or 2pm or 3pm or 1-2pm on 2016-01-02 or 01-02-2016.\"\n",
    "    d5 = \"  --He's a dedicated person. \\t He dedicated his time to work! \\n --are you sure?\"\n",
    "    d6 = \"I am interested in these interesting things that interested me.\"\n",
    "    sample_docs = [d1, d2, d3, d4, d5, d6]\n",
    "\n",
    "    def __init__(self, option='none'):\n",
    "        self.option = option\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def lemmatize(self, text):\n",
    "        tuples = [(str(token), token.lemma_) for token in self.nlp(text)]\n",
    "        text = ' '.join([lemma if lemma != '-PRON-' else token for token, lemma in tuples])\n",
    "        text = re.sub(' ([_-]) ', '\\\\1', text)  # 'short - term' -> 'short-term'\n",
    "        return text\n",
    "\n",
    "    def text_cleanner(self, text):\n",
    "        stopwords = set(['i', 'I', 'you', 'he', 'she', 'it', 'we', 'they', 'am', 'is', 'are'])\n",
    "        text = text.lower()  # 1.lower strings\n",
    "        text = text.replace(\"'s\", 's')  # 2.handle contractions\n",
    "        text = re.sub('([a-zA-Z])\\\\.([a-zA-Z])\\\\.*', '\\\\1\\\\2', text)  # 3. handle abbreviations\n",
    "        text = re.sub('\\\\d+', '', text)  # 4. handle numbers\n",
    "        text = re.sub('[^a-zA-Z0-9\\\\s_-]', ' ', text)  # 5. remove punctuations\n",
    "        if 'lemma' in self.option:\n",
    "            text = self.lemmatize(text)  # 6. perform lemmatization\n",
    "        elif 'stem' in self.option:\n",
    "            text = ' '.join(self.stemmer.stem(x) for x in text.split(' '))  # 6. perform stemming\n",
    "        text = ' '.join(x for x in text.split(' ') if\n",
    "                        x not in stopwords)  # 7. remove stopwords(stopwords类型是set而不是list，因为set的查找速度更快)\n",
    "        text = ' '.join(x.strip('_-') for x in text.split())  # 8. remove spaces,'_' and '-'\n",
    "        return text\n",
    "\n",
    "    def transform(self, docs):  # transformer必须要有fit()和transform(）方法\n",
    "        clean_docs = []\n",
    "        self.fails = []\n",
    "        for doc in tqdm(docs):\n",
    "            try:\n",
    "                clean_docs.append(self.text_cleanner(doc))\n",
    "            except:\n",
    "                self.fails.append(doc)\n",
    "        if len(self.fails) > 0:\n",
    "            print(\"Some documents failed to be converted. Check self.fails for failed documents\")\n",
    "        return clean_docs\n",
    "\n",
    "    def fit(self, docs, y=None):\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, docs, y=None):\n",
    "        return self.fit(docs,y).transform(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 146.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re have abcd or ie usa and and uk and other\n",
      "\n",
      "buy apple good apple from ms abc at dollar pound from monday-friday how s that\n",
      "\n",
      "win t eat of the top cake get dollar or usd and a long-term short-term goal\n",
      "\n",
      "lose pound at or pm or pm or pm or pm on  or \n",
      "\n",
      "hes a dedicated person dedicate his time to work are sure\n",
      "\n",
      "be interested in these interesting thing that interest\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cleanner = TextCleaner(option='lemma')\n",
    "docs = cleanner.sample_docs\n",
    "cleaned_docs = cleanner.transform(docs)\n",
    "for raw,clean in zip(docs, cleaned_docs):\n",
    "    # print('original doc:', raw)\n",
    "    print( clean)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}